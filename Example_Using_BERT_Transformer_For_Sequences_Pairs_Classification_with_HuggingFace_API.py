## CLASSER DES SEQUENCES, AVEC LE TANSFORMER BERT DE LA LIBRAIRIE TANSFORMERS
## DE HUGGINGFACE, SUR LE BACKEND TENSORFLOW ET SON IMPLEMENTATION SPECIFIQUE
## DE L'API KERAS.
## NOUS ALLONS CLASSER CHAQUE PAIRE DE PHRASES EN DEUX CLASSES /
##       LE SENS DE CHAQUE PHRASE DE LA PAIRE EST EQUIVALENT
##               VS. 
##       LE SENS DE CHAQUE PHRASE DE LA PAIRE N'EST PAS EQUIVALENT
## Notre but est de trouver des √©quivalences s√©mantiques. Nous utilisons 
## pour ce faire une t√¢che de classification, attribuant une classe aux paires de sens similaire,
## et une autre classe aux paires de sens diff√©rents.
## Nous aurions pu aussi utiliser des embeddings niveau phrase, en prenant par exemple
## l'embedding contextuel des s√©parateurs [CLS] et [SEP] de BERT, qui pourraient servir
## √† "repr√©senter" les phrases de la paire (cf. https://towardsdatascience.com/bleu-bert-y-comparing-sentence-scores-307e0975994d),
## pour ensuite comparer ces repr√©sentations avec la corr√©lation de Spearman
## (sans doute pr√©f√©rable √† la similarit√© cosinus dans ce cas l√†). 

#Credits to : https://colab.research.google.com/drive/1l39vWjZ5jRUimSQDoUcuWGIoNjLjA2zu
#OS : Windows 10
#Python version 3.7.4
#CUDA version 10.0.130
#Installed Tensorflow CPU 2.0 : pip install tensorflow
#Installed Keras 2.3.1 : pip install Keras - 
#Installed HuggingFace Transformers 2.2.0 : pip install transformers
#Installed Tensorflow example datasets : pip install tensorflow_datasets

#Documentation : https://huggingface.co/transformers/index.html

#Importer Tensorflow
import tensorflow as tf

#Si vous avez Tensorflow CPU ET GPU install√©s, vous pouvez forcer l'utilisation
#de Tensorflow CPU avec la manipulation suivante :
#   Tester que l'environnement a bien acc√®s au(x) GPU(s) - ou pas acc√®s...
print('La GPU est elle disponible : ', tf.test.is_gpu_available(), '\n')
from tensorflow.python.client import device_lib
print('Liste des CPU/GPU disponibles : ', device_lib.list_local_devices(), '\n')
#   Forcer l'utilisation de Tensorflow CPU
import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"    # Voir https://github.com/tensorflow/tensorflow/issues/152
os.environ["CUDA_VISIBLE_DEVICES"]= ""          # Permet de rendre la/les GPU(s) invisibles. Parfois plut√¥t 
                                                # que de laisser vide "", les valeurs "0" ou "-1" sont recommand√©es
#   Tester √† nouveau que l'environnement a bien acc√®s au(x) GPU(s) - ou pas acc√®s...
print('La GPU est elle disponible : ', tf.test.is_gpu_available(), '\n')
print('Liste des CPU/GPU disponibles : ', device_lib.list_local_devices(), '\n')

#Datasets d'exemple, pour la d√©mo d'utilisation des Transformers de HuggingFace
import tensorflow_datasets

# Charger un mod√®le pr√©-entra√Æn√© avec son tokenizer, ici BERT dans sa version TensorFlow (notez que le nom du mod√®le est pr√©fix√© "TF")
from transformers import (TFBertForSequenceClassification, BertTokenizer)
bert_model = TFBertForSequenceClassification.from_pretrained("bert-base-cased") # Charge la conf automatiquement
bert_tokenizer = BertTokenizer.from_pretrained("bert-base-cased")

#   Illustration de la tokenisation
sequence = "Systolic arrays are cool. This üê≥ is cool too."
print("\r\n", "Sequence brute, pour illustrer la tokenisation : ", sequence)
bert_tokenized_sequence = bert_tokenizer.tokenize(sequence)
print("\r\n", "Sequence tokenis√©e pour BERT : ", bert_tokenized_sequence, "\r\n")

#Fine tuner un mod√®le
from transformers import glue_convert_examples_to_features  #GLUE c'est pour General Language Understanding Evaluation (https://gluebenchmark.com/),
                                                            #√áa consiste en un ensemble de ressources, comme des t√¢ches classiques de NLP avec leurs
                                                            #jeux de donn√©es, des donn√©es permettant d'√©valuer ces t√¢ches etc...
                                                            #glue_convert_examples_to_features va nous permettre de convertir le jeu de donn√©es
                                                            #d'exemple de TensorFlow en features, utilisables par la m√©thode "fit" de Keras
                                                            #appliqu√©e au mod√®le de Transformer pr√©fabriqu√© par HuggingFace

#   Charger des donn√©es texte et les pr√©-processer (pipeline d'entr√©e)
#      Importer les donn√©es
data = tensorflow_datasets.load("glue/mrpc")

#      R√©partir les donn√©es en train et test/validation
train_dataset = data["train"]
validation_dataset = data["validation"]

#      Pour illustrer le contenu du jeu de donn√©es d'entra√Ænement, observons son premier √©l√©ment
example = list(train_dataset.__iter__())[0]
print("Exemple : ", example, "\r\n")
print('',
    'idx:      ', example['idx'],       '\n',
    'label:    ', example['label'],     '\n',
    'sentence1:', example['sentence1'], '\n',
    'sentence2:', example['sentence2'], '\n',)
seq0 = example['sentence1'].numpy().decode('utf-8')  #Converti en cha√Æne de caract√®res les bytes r√©cup√©r√©s du tenseur
seq1 = example['sentence2'].numpy().decode('utf-8')  #Converti en cha√Æne de caract√®res les bytes r√©cup√©r√©s du tenseur
print("Premi√®re s√©quence:", seq0, '\n',)
print("Seconde s√©quence:", seq1, '\n',)

#   Encoder les s√©quences au bon format pour notre mod√®le.
#   Deux possibilit√©s : encode()et encode_plus() (qui retourne des informations en plus que nous ne d√©taillerons pas ici)
#      Illustration de encode()
encoded_bert_sequence = bert_tokenizer.encode(seq0, seq1, add_special_tokens=True, max_length=128)
print("S√©quence encod√©e pour BERT : s√©parateur du tokeniseur, cls_token_id:   ",                #Notez qu'ici, 101 est l'ID du Token qui stipule qu'il s'agira
      encoded_bert_sequence, bert_tokenizer.sep_token_id, bert_tokenizer.cls_token_id, '\n')    #d'une t√¢che de classification [CLS], il arrive au d√©but de la s√©quence de tokens.
                                                                                                #102 est quand √† lui l'ID du tiken de s√©paration des phrases, [SEP].
                                                                                                #Le formalisme de BERT pour les paires de phrases est : [CLS] A [SEP] B [SEP]

#      Encodage de nos jeux d'entrainement et de test :
#       Dans notre cas d'exemple, pas besoin de encode() ou encode_plus() car il existe la librairie "glue_convert_examples_to_features"
#       (qui utilise sous cape la m√©thode encode_plus()) qui converti directement les jeux de donn√©es d'exemple de TensorFlow en features
#       exploitables par le mod√®le. De plus cette librairie de d√©pend ni du type de t√¢che (GLUE) ni du tokenizer.
bert_train_dataset = glue_convert_examples_to_features(train_dataset, bert_tokenizer, 128, 'mrpc')          #La t√¢che 'mrpc' se base sur le "Microsoft Research Paraphrase Corpus"
bert_validation_dataset = glue_convert_examples_to_features(validation_dataset, bert_tokenizer, 128, 'mrpc')#(https://www.microsoft.com/en-us/download/details.aspx?id=52398), et 
                                                                                                            #sert √† d√©terminer l'√©quivalence s√©mantique de paires de phrases.

#       On m√©lange les donn√©es (shuffle) et d√©coupe en lots (mini-batchs)
bert_train_dataset = bert_train_dataset.shuffle(100).batch(32).repeat(2)
bert_validation_dataset = bert_validation_dataset.batch(64)

#   On d√©finit les hyper-param√®tres
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

#   Entra√Æner le mod√®le avec la m√©thode "fit" de Keras
#   BERT a d√©j√† √©t√© pr√©alablement pr√©-entrain√©. Il nous est servi d√©j√† pr√©-entra√Æn√© par l'API de HuggingFace.
#   Ici l'entra√Ænement ne fait que fine-tuner, sp√©cialiser (en incr√©mental) le mod√®le (BERT dans notre cas).
print('\n', "Fine-tuning BERT avec MRPC (Microsoft Research Paraphrase Corpus)", '\n')
bert_history = bert_model.fit(bert_train_dataset, epochs=3, validation_data=bert_validation_dataset)

#   Evaluer le mod√®le avec Keras
print('\n', "Evaluating the BERT model", '\n')
bert_model.evaluate(bert_validation_dataset)
